"use strict";(self.webpackChunklibrephotos_docs=self.webpackChunklibrephotos_docs||[]).push([[8171],{3905:(e,t,n)=>{n.d(t,{Zo:()=>d,kt:()=>m});var o=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,o,a=function(e,t){if(null==e)return{};var n,o,a={},i=Object.keys(e);for(o=0;o<i.length;o++)n=i[o],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(o=0;o<i.length;o++)n=i[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var s=o.createContext({}),c=function(e){var t=o.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},d=function(e){var t=c(e.components);return o.createElement(s.Provider,{value:t},e.children)},u="mdxType",p={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},h=o.forwardRef((function(e,t){var n=e.components,a=e.mdxType,i=e.originalType,s=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),u=c(n),h=a,m=u["".concat(s,".").concat(h)]||u[h]||p[h]||i;return n?o.createElement(m,r(r({ref:t},d),{},{components:n})):o.createElement(m,r({ref:t},d))}));function m(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=n.length,r=new Array(i);r[0]=h;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[u]="string"==typeof e?e:a,r[1]=l;for(var c=2;c<i;c++)r[c]=n[c];return o.createElement.apply(null,r)}return o.createElement.apply(null,n)}h.displayName="MDXCreateElement"},2249:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>r,default:()=>p,frontMatter:()=>i,metadata:()=>l,toc:()=>c});var o=n(7462),a=(n(7294),n(3905));const i={title:"\u2601\ufe0f Backend",excerpt:"Development Information regarding LibrePhotos Backend.",last_modified_at:new Date("2020-08-04T00:00:00.000Z"),category:5},r=void 0,l={unversionedId:"development/contribution/backend/index",id:"development/contribution/backend/index",title:"\u2601\ufe0f Backend",description:"The backend uses the following technologies:",source:"@site/docs/development/contribution/backend/index.md",sourceDirName:"development/contribution/backend",slug:"/development/contribution/backend/",permalink:"/docs/development/contribution/backend/",draft:!1,editUrl:"https://github.com/LibrePhotos/librephotos.docs/tree/master/docs/development/contribution/backend/index.md",tags:[],version:"current",frontMatter:{title:"\u2601\ufe0f Backend",excerpt:"Development Information regarding LibrePhotos Backend.",last_modified_at:"2020-08-04T00:00:00.000Z",category:5},sidebar:"userguide",previous:{title:"Contribution",permalink:"/docs/development/contribution/"},next:{title:" \ud83d\udcc1 Upload",permalink:"/docs/development/contribution/backend/upload"}},s={},c=[{value:"\u2728 Code Standards",id:"-code-standards",level:2},{value:"\ud83d\udc1b Debugging",id:"-debugging",level:2},{value:"Using pdb",id:"using-pdb",level:3},{value:"Using silk",id:"using-silk",level:3},{value:"\ud83c\udfd9\ufe0f Structure",id:"\ufe0f-structure",level:2},{value:"Django",id:"django",level:3},{value:"management",id:"management",level:4},{value:"migrations",id:"migrations",level:4},{value:"models",id:"models",level:4},{value:"views",id:"views",level:4},{value:"serializer",id:"serializer",level:4},{value:"Machine Learning",id:"machine-learning",level:3},{value:"im2txt",id:"im2txt",level:4},{value:"Face Recognition",id:"face-recognition",level:4},{value:"places365",id:"places365",level:4},{value:"Semantic Search",id:"semantic-search",level:4}],d={toc:c},u="wrapper";function p(e){let{components:t,...n}=e;return(0,a.kt)(u,(0,o.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("p",null,"The backend uses the following technologies:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Python"),(0,a.kt)("li",{parentName:"ul"},"Django"),(0,a.kt)("li",{parentName:"ul"},"Pytorch")),(0,a.kt)("h2",{id:"-code-standards"},"\u2728 Code Standards"),(0,a.kt)("p",null,"In order to have a similar structure to the code, the backend repository has a pre-commit hook. Just use pre-commit install in the folder and then the linter and the formatter will check it before you commit your actual work."),(0,a.kt)("p",null,"We use black, flake8 and isort to keep our code tidy."),(0,a.kt)("h2",{id:"-debugging"},"\ud83d\udc1b Debugging"),(0,a.kt)("p",null,"Usually nothing goes as planned, and you need to debug your shiny new feature. In the next paragraph, I will explain to you the two tools we have for debugging:"),(0,a.kt)("h3",{id:"using-pdb"},"Using pdb"),(0,a.kt)("p",null,"Add the following in the python code where you want a breakpoint:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"import pdb; pdb.set_trace()\n")),(0,a.kt)("p",null,"Attach to the backend service:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"docker attach $(docker ps --filter name=backend -q)\n")),(0,a.kt)("p",null,"Debug as normal in pdb!"),(0,a.kt)("p",null,"When you're done debugging, continue execution (c) and press Ctrl-P followed by Ctrl-Q to detach from the container without stopping it."),(0,a.kt)("h3",{id:"using-silk"},"Using silk"),(0,a.kt)("p",null,"In order to debug queries, start the backend container in dev mode. Then you can access silk under /api/silk. Silk is a live profiling and inspection tool for the Django framework. Silk intercepts and stores HTTP requests and database queries before presenting them in a user interface for further inspection."),(0,a.kt)("h2",{id:"\ufe0f-structure"},"\ud83c\udfd9\ufe0f Structure"),(0,a.kt)("p",null,"There are a lot of folders in our backend. Here is a quick rundown on where you can find what."),(0,a.kt)("h3",{id:"django"},"Django"),(0,a.kt)("p",null,"Most of the application code is within the API folder using Django. In the following section, I will explain where you can find what"),(0,a.kt)("h4",{id:"management"},"management"),(0,a.kt)("p",null,"This exposes all the commands that you can use via the command line. If you want a new command, that's the place to add it."),(0,a.kt)("h4",{id:"migrations"},"migrations"),(0,a.kt)("p",null,"Every time we change our models, we have to migrate the database. We use Django migration feature to create migration files to migrate without the headaches."),(0,a.kt)("h4",{id:"models"},"models"),(0,a.kt)("p",null,"Here are the actual data types. If you want to figure out how a photo works or how faces are connected to persons, then this is your folder."),(0,a.kt)("h4",{id:"views"},"views"),(0,a.kt)("p",null,"Here you can find our API implemented. They are separated, similar to the models. Views that expose the photos will be here in photos too."),(0,a.kt)("h4",{id:"serializer"},"serializer"),(0,a.kt)("p",null,"You have your python model and want to somehow convert that to JSON. That's what the serializer does! There are two different types of serializers: normal ones and serpy serializer. Serpy serializer is faster, and we sometimes need it if we need to serialize a lot of data. The package is no longer maintained, though, and we are looking for a replacement or for refactoring to serialize fewer data at once."),(0,a.kt)("p",null,"We are currently in the process of splitting them up, similar to how we did it in views."),(0,a.kt)("h3",{id:"machine-learning"},"Machine Learning"),(0,a.kt)("p",null,"We use as a base framework PyTorch. If you find a cool machine learning model with PyTorch, we sure can add that too."),(0,a.kt)("h4",{id:"im2txt"},"im2txt"),(0,a.kt)("p",null,"im2txt is an image captioning package which allows us to generate captions on demand. This sometimes creates useful output, but it is kind of old and there should be more recent models"),(0,a.kt)("h4",{id:"face-recognition"},"Face Recognition"),(0,a.kt)("p",null,"We use dlib and face_recognition to detect faces. A very cool feature would be the automatic clustering of unknown faces, which we have not implemented yet."),(0,a.kt)("h4",{id:"places365"},"places365"),(0,a.kt)("p",null,"places365 generates scene classifications for a given image. It generates the tags you see when you open the photo details in the UI."),(0,a.kt)("h4",{id:"semantic-search"},"Semantic Search"),(0,a.kt)("p",null,'Here you can find the code which allows us to search semantically for images like "trees in a valley".'))}p.isMDXComponent=!0}}]);