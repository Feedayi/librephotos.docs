"use strict";(self.webpackChunklibrephotos_docs=self.webpackChunklibrephotos_docs||[]).push([[8295],{3905:(e,t,i)=>{i.d(t,{Zo:()=>d,kt:()=>u});var n=i(7294);function r(e,t,i){return t in e?Object.defineProperty(e,t,{value:i,enumerable:!0,configurable:!0,writable:!0}):e[t]=i,e}function a(e,t){var i=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),i.push.apply(i,n)}return i}function o(e){for(var t=1;t<arguments.length;t++){var i=null!=arguments[t]?arguments[t]:{};t%2?a(Object(i),!0).forEach((function(t){r(e,t,i[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(i)):a(Object(i)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(i,t))}))}return e}function s(e,t){if(null==e)return{};var i,n,r=function(e,t){if(null==e)return{};var i,n,r={},a=Object.keys(e);for(n=0;n<a.length;n++)i=a[n],t.indexOf(i)>=0||(r[i]=e[i]);return r}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(n=0;n<a.length;n++)i=a[n],t.indexOf(i)>=0||Object.prototype.propertyIsEnumerable.call(e,i)&&(r[i]=e[i])}return r}var c=n.createContext({}),l=function(e){var t=n.useContext(c),i=t;return e&&(i="function"==typeof e?e(t):o(o({},t),e)),i},d=function(e){var t=l(e.components);return n.createElement(c.Provider,{value:t},e.children)},p="mdxType",h={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var i=e.components,r=e.mdxType,a=e.originalType,c=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),p=l(i),m=r,u=p["".concat(c,".").concat(m)]||p[m]||h[m]||a;return i?n.createElement(u,o(o({ref:t},d),{},{components:i})):n.createElement(u,o({ref:t},d))}));function u(e,t){var i=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var a=i.length,o=new Array(a);o[0]=m;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s[p]="string"==typeof e?e:r,o[1]=s;for(var l=2;l<a;l++)o[l]=i[l];return n.createElement.apply(null,o)}return n.createElement.apply(null,i)}m.displayName="MDXCreateElement"},3188:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>l});var n=i(7462),r=(i(7294),i(3905));const a={title:"\ud83d\udcdd Image Captioning",excerpt:"What is image captioning and how do I use it?",sidebar_position:6},o=void 0,s={unversionedId:"user-guide/image-captioning",id:"user-guide/image-captioning",title:"\ud83d\udcdd Image Captioning",description:"What is image captioning?",source:"@site/docs/user-guide/image-captioning.md",sourceDirName:"user-guide",slug:"/user-guide/image-captioning",permalink:"/docs/user-guide/image-captioning",draft:!1,editUrl:"https://github.com/LibrePhotos/librephotos.docs/tree/master/docs/user-guide/image-captioning.md",tags:[],version:"current",sidebarPosition:6,frontMatter:{title:"\ud83d\udcdd Image Captioning",excerpt:"What is image captioning and how do I use it?",sidebar_position:6},sidebar:"userguide",previous:{title:"\ud83d\udca1 Feature Comparison",permalink:"/docs/user-guide/features"},next:{title:"\u2753 FAQ",permalink:"/docs/user-guide/faq"}},c={},l=[{value:"What is image captioning?",id:"what-is-image-captioning",level:2},{value:"How do I change the model?",id:"how-do-i-change-the-model",level:2},{value:"What is the difference between the models?",id:"what-is-the-difference-between-the-models",level:2},{value:"im2txt PyTorch",id:"im2txt-pytorch",level:3},{value:"im2txt ONNX",id:"im2txt-onnx",level:3},{value:"Blip Base Capfilt Large",id:"blip-base-capfilt-large",level:3}],d={toc:l},p="wrapper";function h(e){let{components:t,...i}=e;return(0,r.kt)(p,(0,n.Z)({},d,i,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h2",{id:"what-is-image-captioning"},"What is image captioning?"),(0,r.kt)("p",null,"The goal of automatic image captioning to understand the content of an image and then produce a coherent and contextually relevant sentence or phrase that describes what is happening in the image."),(0,r.kt)("p",null,"To use the feature in LibrePhotos, open up an image, click on the information icon and then click on the generate button below the caption segment. It should generate a phrase, which should describe your image."),(0,r.kt)("h2",{id:"how-do-i-change-the-model"},"How do I change the model?"),(0,r.kt)("p",null,"Click on your avatar in the top right, and go to ",(0,r.kt)("inlineCode",{parentName:"p"},"Admin Area"),". There is a setting for ",(0,r.kt)("inlineCode",{parentName:"p"},"Captioning Model"),". You can select here between the different models. After selecting a model, it will be downloaded and added to your data_models folder."),(0,r.kt)("h2",{id:"what-is-the-difference-between-the-models"},"What is the difference between the models?"),(0,r.kt)("p",null,'Currently, there are three available models: "im2txt PyTorch," "im2txt Onnx," and "Blip."'),(0,r.kt)("h3",{id:"im2txt-pytorch"},"im2txt PyTorch"),(0,r.kt)("p",null,"This model serves as the default choice. It offers rapid results and represents the original implementation of the image captioning task. It uses the PyTorch deep learning framework, it has been a reliable option for users seeking both speed and baseline performance."),(0,r.kt)("h3",{id:"im2txt-onnx"},"im2txt ONNX"),(0,r.kt)("p",null,'Utilizing the Open Neural Network Exchange (ONNX) as its engine, "im2txt Onnx" is designed for enhanced efficiency during inference. It exhibits a slight speed improvement compared to the original PyTorch model. The integration of ONNX facilitates seamless deployment across different platforms.'),(0,r.kt)("h3",{id:"blip-base-capfilt-large"},"Blip Base Capfilt Large"),(0,r.kt)("p",null,'The next generation model "Blip" excels in providing highly accurate image descriptions. However, it comes with a trade-off, as it operates at approximately 20 times slower speeds than "im2txt PyTorch." This deliberate sacrifice in speed is made to achieve superior descriptive accuracy, making "Blip" an ideal choice for applications prioritizing precision over real-time processing.'),(0,r.kt)("p",null,"Users can choose a model based on their specific requirements, balancing the need for speed, accuracy, and the trade-offs associated with each implementation. It's recommended to consider the performance of your system and the desired performance characteristics when selecting the most suitable model."))}h.isMDXComponent=!0}}]);